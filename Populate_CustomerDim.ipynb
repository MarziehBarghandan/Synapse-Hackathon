{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\r\n",
        "import pyspark.sql.types "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%pyspark\r\n",
        "# Reading data from Customer table, select columns that we will use.\r\n",
        "CustomerDF = spark.sql(\"SELECT CustomerId, CustomerEstablishedDate, CustomerTypeId, LedgerId  FROM `WWI_Hack`.`Customer` \")\r\n",
        "CustomerDF.show(10)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading data from LegalEntityCustomer table, select columns that we will use.\r\n",
        "LECustomerDF = spark.sql(\"SELECT CustomerId,LegalEntityName,LegalEntityDateOfEstablishment,StateOfLegalEntityEstablishment FROM `WWI_Hack`.`LegalEntityCustomer` \")\r\n",
        "LECustomerDF.show(10)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {}
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Joining customer data \r\n",
        "inner_join = CustomerDF.alias(\"a\").join(LECustomerDF.alias(\"b\"), CustomerDF.CustomerId == LECustomerDF.CustomerId).select(\"a.*\",\"b.LegalEntityName\",\"b.LegalEntityDateOfEstablishment\",\"b.StateOfLegalEntityEstablishment\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check schema for mismatch with CustomerDim table\r\n",
        "inner_join.printSchema()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Casting TimestampType\r\n",
        "inner_join=inner_join.withColumn(\"CustomerEstablishedDate\",col(\"CustomerEstablishedDate\").cast(TimestampType()))\\\r\n",
        "                     .withColumn(\"LegalEntityDateOfEstablishment\",col(\"LegalEntityDateOfEstablishment\").cast(TimestampType())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a schema to match CustomerDim table\r\n",
        "CustomerDimSchema =    [StructField('CustomerId',IntegerType(),False),\\\r\n",
        "                        StructField('CustomerEstablishedDate',TimestampType(),True),\\\r\n",
        "                        StructField('CustomerTypeId',IntegerType(),True),\\\r\n",
        "                        StructField('LedgerId',IntegerType(),True),\\\r\n",
        "                        StructField('LegalEntityName',StringType(),True),\\\r\n",
        "                        StructField('LegalEntityDateOfEstablishment',TimestampType(),True),\\\r\n",
        "                        StructField('StateOfLegalEntityEstablishment',StringType(),True)]\r\n",
        "CustomerDim = sqlContext.createDataFrame(inner_join.rdd, StructType(CustomerDimSchema))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CustomerDim.show(10)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write  using AAD Auth to internal table\r\n",
        "# Add required imports\r\n",
        "import com.microsoft.spark.sqlanalytics\r\n",
        "from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
        "\r\n",
        "# Configure and submit the request to write to Synapse Dedicated SQL Pool\r\n",
        "# Sample below is using AAD-based authentication approach; See further examples to leverage SQL Basic auth.\r\n",
        "(CustomerDim.write\r\n",
        " # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
        " # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
        " .option(Constants.SERVER, \"synapsedwhdemo.sql.azuresynapse.net\")\r\n",
        " # Choose a save mode that is apt for your use case.\r\n",
        " # Options for save modes are \"error\" or \"errorifexists\" (default), \"overwrite\", \"append\", \"ignore\".\r\n",
        " # refer to https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes\r\n",
        " .mode(\"overwrite\")\r\n",
        " # Required parameter - Three-part table name to which data will be written\r\n",
        " .synapsesql(\"SqlPool01.WWI.CustomerDim\"))\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}